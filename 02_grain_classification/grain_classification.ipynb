{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import timm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")"
      ],
      "metadata": {
        "id": "zc7ptOcA8wAv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "\n",
        "    # Dataset Paths\n",
        "    BASE_PATH = '/kaggle/input/grain-classification'\n",
        "    TRAIN_DIR = f'{BASE_PATH}/train-2/train-2/images'\n",
        "    TEST_DIR = f'{BASE_PATH}/test-2/test-2/images'\n",
        "    TRAIN_CSV = f'{BASE_PATH}/train-2/train-2/train.csv'\n",
        "    TEST_CSV = f'{BASE_PATH}/test-2/test-2/test.csv'\n",
        "\n",
        "    # Model Architecture\n",
        "    MODELS = [\n",
        "        'tf_efficientnetv2_m.in21k_ft_in1k',\n",
        "        'convnext_base.fb_in22k_ft_in1k'\n",
        "    ]\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    IMG_SIZE = 384\n",
        "    BATCH_SIZE = 32\n",
        "    N_FOLDS = 5\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 1e-5\n",
        "    ACCUMULATION_STEPS = 1\n",
        "    LABEL_SMOOTHING = 0.1\n",
        "\n",
        "    # System Configuration\n",
        "    NUM_WORKERS = 2\n",
        "    RANDOM_SEED = 42\n",
        "\n",
        "    # Class Information\n",
        "    CLASS_NAMES = ['barley', 'flax', 'oats', 'wheat']\n",
        "    NUM_CLASSES = len(CLASS_NAMES)\n",
        "    CLASS_TO_IDX = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
        "    IDX_TO_CLASS = {idx: name for name, idx in CLASS_TO_IDX.items()}\n",
        "\n",
        "# Display configuration\n",
        "print(\"Configuration Summary:\")\n",
        "print(f\"  Models: {len(Config.MODELS)}\")\n",
        "print(f\"  Classes: {Config.NUM_CLASSES}\")\n",
        "print(f\"  Image Size: {Config.IMG_SIZE}×{Config.IMG_SIZE}\")\n",
        "print(f\"  Batch Size: {Config.BATCH_SIZE}\")\n",
        "print(f\"  Cross-Validation: {Config.N_FOLDS}-fold\")\n",
        "print(f\"  Epochs: {Config.EPOCHS}\")"
      ],
      "metadata": {
        "id": "EUkpaw728wAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    \"\"\"Set random seed for reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def clear_memory():\n",
        "    \"\"\"Clear GPU memory cache.\"\"\"\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "def get_device():\n",
        "    \"\"\"Get available device and display information.\"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"Device: {device}\")\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    else:\n",
        "        print(\"Device: CPU (GPU not available)\")\n",
        "\n",
        "    return device\n",
        "\n",
        "# Initialize\n",
        "set_seed(Config.RANDOM_SEED)\n",
        "device = get_device()"
      ],
      "metadata": {
        "id": "y0q0zSQl8wAx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Data Loading & Preprocessing <a id='data'></a>"
      ],
      "metadata": {
        "id": "u6FJnc7o8wAy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Load Data"
      ],
      "metadata": {
        "id": "jEls_s928wAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset():\n",
        "    \"\"\"Load and prepare training and test datasets.\"\"\"\n",
        "\n",
        "    # Load CSVs\n",
        "    train_df = pd.read_csv(Config.TRAIN_CSV)\n",
        "    test_df = pd.read_csv(Config.TEST_CSV)\n",
        "\n",
        "    # Add full paths\n",
        "    train_df['path'] = train_df['img'].apply(lambda x: os.path.join(Config.TRAIN_DIR, x))\n",
        "    test_df['path'] = test_df['img'].apply(lambda x: os.path.join(Config.TEST_DIR, x))\n",
        "\n",
        "    # Encode labels for training data\n",
        "    train_df['label'] = train_df['class'].map(Config.CLASS_TO_IDX)\n",
        "\n",
        "    # Display statistics\n",
        "    print(\"Dataset Statistics:\")\n",
        "    print(f\"  Training samples: {len(train_df)}\")\n",
        "    print(f\"  Test samples: {len(test_df)}\")\n",
        "    print(f\"\\nClass distribution (training):\")\n",
        "    print(train_df['class'].value_counts())\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "train_df, test_df = load_dataset()"
      ],
      "metadata": {
        "id": "jABwsK_c8wA2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def get_transforms(img_size=384, augment=True):\n",
        "\n",
        "    if augment:\n",
        "        return A.Compose([\n",
        "            # Geometric transformations\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.RandomRotate90(p=0.5),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.VerticalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(\n",
        "                shift_limit=0.0625,\n",
        "                scale_limit=0.15,\n",
        "                rotate_limit=45,\n",
        "                p=0.5\n",
        "            ),\n",
        "\n",
        "            # Blur augmentations\n",
        "            A.OneOf([\n",
        "                A.MotionBlur(p=0.2),\n",
        "                A.MedianBlur(blur_limit=3, p=0.1),\n",
        "                A.GaussianBlur(p=0.2),\n",
        "            ], p=0.3),\n",
        "\n",
        "            # Distortion\n",
        "            A.OneOf([\n",
        "                A.OpticalDistortion(p=0.3),\n",
        "                A.GridDistortion(p=0.3),\n",
        "            ], p=0.3),\n",
        "\n",
        "            # Color augmentations\n",
        "            A.OneOf([\n",
        "                A.HueSaturationValue(\n",
        "                    hue_shift_limit=20,\n",
        "                    sat_shift_limit=30,\n",
        "                    val_shift_limit=20,\n",
        "                    p=0.3\n",
        "                ),\n",
        "                A.RandomBrightnessContrast(\n",
        "                    brightness_limit=0.2,\n",
        "                    contrast_limit=0.2,\n",
        "                    p=0.3\n",
        "                ),\n",
        "            ], p=0.5),\n",
        "\n",
        "            # Cutout\n",
        "            A.CoarseDropout(\n",
        "                max_holes=8,\n",
        "                max_height=32,\n",
        "                max_width=32,\n",
        "                p=0.3\n",
        "            ),\n",
        "\n",
        "            # Normalization\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "    else:\n",
        "        # Validation/Test transforms (no augmentation)\n",
        "        return A.Compose([\n",
        "            A.Resize(img_size, img_size),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "print(\"✓ Augmentation transforms defined\")"
      ],
      "metadata": {
        "id": "kw92OTmQ8wA2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Dataset Class"
      ],
      "metadata": {
        "id": "wAZ6q8Zv8wA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrainDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, transform=None):\n",
        "        self.df = dataframe.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "        self.has_labels = 'label' in dataframe.columns\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        image = cv2.imread(row['path'])\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image=image)['image']\n",
        "\n",
        "        if self.has_labels:\n",
        "            label = torch.tensor(row['label'], dtype=torch.long)\n",
        "            return image, label\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "print(\"✓ Dataset class defined\")"
      ],
      "metadata": {
        "id": "X_aquy398wA2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Create Cross-Validation Folds"
      ],
      "metadata": {
        "id": "b-L8gj9c8wA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_folds(df, n_folds=5):\n",
        "    df = df.copy()\n",
        "    df['fold'] = -1\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=Config.RANDOM_SEED)\n",
        "\n",
        "    for fold, (_, valid_idx) in enumerate(skf.split(df, df['label'])):\n",
        "        df.loc[valid_idx, 'fold'] = fold\n",
        "\n",
        "    print(f\"Created {n_folds}-fold stratified cross-validation\")\n",
        "    print(f\"\\nFold distribution:\")\n",
        "    print(df.groupby(['fold', 'class']).size().unstack(fill_value=0))\n",
        "\n",
        "    return df\n",
        "\n",
        "train_df = create_folds(train_df, Config.N_FOLDS)"
      ],
      "metadata": {
        "id": "cuN3KSjD8wA2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 4. Model Architecture <a id='model'></a>"
      ],
      "metadata": {
        "id": "wLpx6JVH8wA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GrainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name, num_classes=4, pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pre-trained backbone (without classifier)\n",
        "        self.backbone = timm.create_model(\n",
        "            model_name,\n",
        "            pretrained=pretrained,\n",
        "            num_classes=0  # Remove original classifier\n",
        "        )\n",
        "\n",
        "        n_features = self.backbone.num_features\n",
        "\n",
        "        # Custom classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(n_features, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        output = self.classifier(features)\n",
        "        return output\n",
        "\n",
        "print(\"✓ Model architecture defined\")"
      ],
      "metadata": {
        "id": "bUYThkGs8wA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, criterion, optimizer, scaler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        predictions = outputs.argmax(dim=1).cpu().numpy()\n",
        "        all_predictions.extend(predictions)\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_f1\n",
        "\n",
        "\n",
        "def validate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(loader, desc='Validation'):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            predictions = outputs.argmax(dim=1).cpu().numpy()\n",
        "            all_predictions.extend(predictions)\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_loss = running_loss / len(loader.dataset)\n",
        "    epoch_f1 = f1_score(all_labels, all_predictions, average='macro')\n",
        "\n",
        "    return epoch_loss, epoch_f1\n",
        "\n",
        "print(\"✓ Training functions defined\")"
      ],
      "metadata": {
        "id": "PyQdrM3D8wA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Main Training Loop"
      ],
      "metadata": {
        "id": "1vDVFVG28wA3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model_name, train_df, fold, epochs):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {model_name} - Fold {fold+1}/{Config.N_FOLDS}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    clear_memory()\n",
        "\n",
        "    train_fold = train_df[train_df['fold'] != fold]\n",
        "    valid_fold = train_df[train_df['fold'] == fold]\n",
        "\n",
        "    train_dataset = GrainDataset(train_fold, get_transforms(Config.IMG_SIZE, augment=True))\n",
        "    valid_dataset = GrainDataset(valid_fold, get_transforms(Config.IMG_SIZE, augment=False))\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    model = GrainClassifier(model_name, Config.NUM_CLASSES).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=Config.LABEL_SMOOTHING)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=Config.LEARNING_RATE,\n",
        "        weight_decay=Config.WEIGHT_DECAY\n",
        "    )\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "        optimizer,\n",
        "        T_0=10,\n",
        "        T_mult=1\n",
        "    )\n",
        "\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # Training loop\n",
        "    best_f1 = 0\n",
        "    model_short = model_name.replace('.', '_').replace('/', '_')\n",
        "    best_model_path = f'{model_short}_fold{fold}_best.pth'\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        train_loss, train_f1 = train_one_epoch(\n",
        "            model, train_loader, criterion, optimizer, scaler, device\n",
        "        )\n",
        "\n",
        "        valid_loss, valid_f1 = validate(model, valid_loader, criterion, device)\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f}, Train F1: {train_f1:.4f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.4f}, Valid F1: {valid_f1:.4f}\")\n",
        "\n",
        "        if valid_f1 > best_f1:\n",
        "            best_f1 = valid_f1\n",
        "            torch.save(model.state_dict(), best_model_path)\n",
        "            print(f\"✓ Best model saved (F1: {best_f1:.4f})\")\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Fold {fold+1} completed. Best F1: {best_f1:.4f}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    return best_model_path, best_f1\n",
        "\n",
        "print(\"✓ Main training function defined\")"
      ],
      "metadata": {
        "id": "6Ftx4Bji8wA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def train_all_models(train_df, models, n_folds, epochs):\n",
        "    results = {}\n",
        "\n",
        "    for model_name in models:\n",
        "        print(f\"\\n{'#'*60}\")\n",
        "        print(f\"MODEL: {model_name}\")\n",
        "        print(f\"{'#'*60}\")\n",
        "\n",
        "        results[model_name] = {'paths': [], 'scores': []}\n",
        "\n",
        "        for fold in range(n_folds):\n",
        "            path, score = train_model(model_name, train_df, fold, epochs)\n",
        "            results[model_name]['paths'].append(path)\n",
        "            results[model_name]['scores'].append(score)\n",
        "\n",
        "        avg_score = np.mean(results[model_name]['scores'])\n",
        "        std_score = np.std(results[model_name]['scores'])\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Model: {model_name}\")\n",
        "        print(f\"Average F1: {avg_score:.4f} ± {std_score:.4f}\")\n",
        "        print(f\"Fold scores: {[f'{s:.4f}' for s in results[model_name]['scores']]}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "training_results = train_all_models(\n",
        "    train_df,\n",
        "    Config.MODELS,\n",
        "    Config.N_FOLDS,\n",
        "    Config.EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "KANZLGV78wA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_tta(model, images, device):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Original\n",
        "        outputs = model(images)\n",
        "        predictions.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "        # Horizontal flip\n",
        "        outputs = model(torch.flip(images, dims=[3]))\n",
        "        predictions.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "        # Vertical flip\n",
        "        outputs = model(torch.flip(images, dims=[2]))\n",
        "        predictions.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "        # Both flips\n",
        "        outputs = model(torch.flip(images, dims=[2, 3]))\n",
        "        predictions.append(F.softmax(outputs, dim=1).cpu().numpy())\n",
        "\n",
        "    return np.mean(predictions, axis=0)\n",
        "\n",
        "print(\"✓ TTA function defined\")"
      ],
      "metadata": {
        "id": "FJrUsqRu8wA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def make_ensemble_predictions(test_df, model_results):\n",
        "    test_dataset = GrainDataset(test_df, get_transforms(Config.IMG_SIZE, augment=False))\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=Config.BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        num_workers=Config.NUM_WORKERS,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    all_model_predictions = []\n",
        "\n",
        "    for model_name, results in model_results.items():\n",
        "        print(f\"\\nGenerating predictions: {model_name}\")\n",
        "        model_fold_predictions = []\n",
        "\n",
        "        for fold, path in enumerate(results['paths']):\n",
        "            clear_memory()\n",
        "\n",
        "            model = GrainClassifier(model_name, Config.NUM_CLASSES).to(device)\n",
        "            model.load_state_dict(torch.load(path))\n",
        "            model.eval()\n",
        "\n",
        "            fold_predictions = []\n",
        "            for images in tqdm(test_loader, desc=f\"  Fold {fold+1}\"):\n",
        "                preds = predict_with_tta(model, images, device)\n",
        "                fold_predictions.append(preds)\n",
        "\n",
        "            fold_predictions = np.vstack(fold_predictions)\n",
        "            model_fold_predictions.append(fold_predictions)\n",
        "\n",
        "            del model\n",
        "            clear_memory()\n",
        "\n",
        "        model_avg = np.mean(model_fold_predictions, axis=0)\n",
        "        all_model_predictions.append(model_avg)\n",
        "\n",
        "    ensemble_predictions = np.mean(all_model_predictions, axis=0)\n",
        "    final_classes = ensemble_predictions.argmax(axis=1)\n",
        "\n",
        "    return final_classes, ensemble_predictions\n",
        "\n",
        "print(\"✓ Ensemble prediction function defined\")"
      ],
      "metadata": {
        "id": "djWtBbJQ8wA3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "final_predictions, ensemble_probs = make_ensemble_predictions(test_df, training_results)\n",
        "\n",
        "submission = pd.DataFrame({\n",
        "    'img': test_df['img'],\n",
        "    'class': [Config.IDX_TO_CLASS[idx] for idx in final_predictions]\n",
        "})\n",
        "\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "_ykaDNwN8wA4"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}